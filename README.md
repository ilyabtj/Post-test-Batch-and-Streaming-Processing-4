# Kafka + Spark Structured Streaming Pipeline

## ðŸ“Œ Project Overview
This project demonstrates a **data pipeline** built with **Apache Kafka** and **Apache Spark Structured Streaming**.  
The pipeline simulates a real-time data flow starting from message generation, ingestion into Kafka, processing with Spark, and producing multiple outputs.

---

## ðŸ”„ Pipeline Flow
1. **Data Generator** (`generator.py`)  
   - Creates nested JSON messages (user info, transaction info, and items).  

2. **Producer** (`producer.py`)  
   - Sends generated messages to Kafka topic `ilya_input`.  

3. **Consumer + Transformer** (`spark_job.py`)  
   - Reads nested JSON messages from Kafka.  
   - Flattens the structure into a tabular schema.  
   - Produces two outputs:
     - **Flattened messages** (sent back to Kafka topic `ilya_flattened`).
     - **Aggregated data** (count, sum, average transaction amounts per user).  

4. **Sink**  
   - Flattened â†’ Kafka topic `ilya_flattened`.  
   - Aggregated â†’ Console sink (can be extended to file or database).  

---

## ðŸ—‚ Project Structure
kafka-spark-pipeline/
â”‚
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ generator.py # Data generator (nested JSON)
â”‚ â”œâ”€â”€ producer.py # Kafka producer
â”‚ â”œâ”€â”€ spark_job.py # Spark consumer + transformer + sinks
â”‚ â””â”€â”€ requirements.txt # Python dependencies
â”‚
â”œâ”€â”€ docker-compose.yml # Kafka + Zookeeper setup
â””â”€â”€ README.md # Project documentation


# How To Run
## 1. Start Kafka & Zookeeper
``` bash
docker-compose up -d
```